{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_s = pd.read_csv('inference_preembed_result.csv',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGender(x):\n",
    "    if x>0.5:\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>True_label</th>\n",
       "      <th>Predicted_label</th>\n",
       "      <th>ReviewerID</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>AUCXT9K30SHYF</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>AUCXT9K30SHYF</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>AUCXT9K30SHYF</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>AUCXT9K30SHYF</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>AUCXT9K30SHYF</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   True_label  Predicted_label     ReviewerID  gender\n",
       "0           1                1  AUCXT9K30SHYF       1\n",
       "1           1                0  AUCXT9K30SHYF       1\n",
       "2           1                1  AUCXT9K30SHYF       1\n",
       "3           1                0  AUCXT9K30SHYF       1\n",
       "4           1                1  AUCXT9K30SHYF       1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_gender.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precisionRecallF1(conf_mat): \n",
    "    \n",
    "    male_pr = conf_mat[0][0]/(conf_mat[0][0]+conf_mat[1][0])\n",
    "    male_r = conf_mat[0][0]/(conf_mat[0][0]+conf_mat[0][1])\n",
    "    \n",
    "    female_pr = conf_mat[1][1]/(conf_mat[1][1]+conf_mat[0][1])\n",
    "    female_r = conf_mat[1][1]/(conf_mat[1][1]+conf_mat[1][0])\n",
    "    \n",
    "    male_f1 = 2*(male_pr*male_r)/(male_pr + male_r)\n",
    "    female_f1 = 2*(female_pr*female_r)/(female_pr + female_r)\n",
    "    \n",
    "    return male_pr, male_r, male_f1, female_pr, female_r, female_f1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(fname): # input: csv file, calculates accuracy, f1-score w/o majority voting\n",
    "    \n",
    "    df_s = pd.read_csv(fname)\n",
    "    \n",
    "    df_gender = df_s.groupby('ReviewerID').agg({'Predicted_label':'sum','True_label':'count'}). \\\n",
    "reset_index().rename(columns={'Predicted_label':'label_sum','True_label':'l_count'})\n",
    "    \n",
    "    df_gender['gender'] = df_gender['label_sum']/df_gender['l_count']\n",
    "    \n",
    "    df_gender['gender'] = df_gender['gender'].apply(lambda x:getGender(x))\n",
    "    \n",
    "    df_gender = df_gender.drop(columns=['label_sum','l_count'])\n",
    "    \n",
    "    df_all_gender = pd.merge(df_s,df_gender,on='ReviewerID')\n",
    "    \n",
    "    acc = accuracy_score(df_all_gender.True_label,df_all_gender.Predicted_label) \n",
    "    \n",
    "    acc_mv = accuracy_score(df_all_gender.True_label,df_all_gender.gender)\n",
    "    \n",
    "    conf_mat_mv = confusion_matrix(df_all_gender.True_label,df_all_gender.gender)\n",
    "    \n",
    "    conf_mat = confusion_matrix(df_all_gender.True_label,df_all_gender.Predicted_label)\n",
    "    \n",
    "    male_pr_mv, male_r_mv, male_f1_mv, female_pr_mv, female_r_mv, female_f1_mv = precisionRecallF1(conf_mat_mv)\n",
    "    \n",
    "    male_pr, male_r, male_f1, female_pr, female_r, female_f1 = precisionRecallF1(conf_mat)\n",
    "    \n",
    "    print('without majority voting results -')\n",
    "    print(f'accuracy: {acc}, male f1: {male_f1}, female f1: {female_f1}')\n",
    "    \n",
    "    print('with majority voting results -')\n",
    "    print(f'accuracy: {acc_mv}, male f1: {male_f1_mv}, female f1: {female_f1_mv}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "without majority voting results -\n",
      "accuracy: 0.7251947642525259, male f1: 0.7120841051861675, female f1: 0.7371634019328063\n",
      "with majority voting results -\n",
      "accuracy: 0.8012269391087037, male f1: 0.7965013105056868, female f1: 0.8057380724414493\n"
     ]
    }
   ],
   "source": [
    "evaluation('inference_preembed_result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "without majority voting results -\n",
      "accuracy: 0.7336565572632477, male f1: 0.7218600729240505, female f1: 0.7444931273779037\n",
      "with majority voting results -\n",
      "accuracy: 0.8109789833112898, male f1: 0.8071954614496337, female f1: 0.8146168703093878\n"
     ]
    }
   ],
   "source": [
    "evaluation('inference_result_RNN_vanilla_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "without majority voting results -\n",
      "accuracy: 0.7254032684854282, male f1: 0.7171541336853197, female f1: 0.7331848705234507\n",
      "with majority voting results -\n",
      "accuracy: 0.8025948600332903, male f1: 0.8013855659988811, female f1: 0.803789517246777\n"
     ]
    }
   ],
   "source": [
    "evaluation('../HAN/output_han_amazon.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "without majority voting results -\n",
      "accuracy: 0.7520313359329179, male f1: 0.7493713654955585, female f1: 0.7546354377477685\n",
      "with majority voting results -\n",
      "accuracy: 0.8211777201203803, male f1: 0.82277158711336, female f1: 0.8195549247411497\n"
     ]
    }
   ],
   "source": [
    "evaluation('../amazonUser/output_Char_cnn_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "without majority voting results -\n",
      "accuracy: 0.7520313359329179, male f1: 0.7493713654955585, female f1: 0.7546354377477685\n",
      "with majority voting results -\n",
      "accuracy: 0.8211777201203803, male f1: 0.82277158711336, female f1: 0.8195549247411497\n"
     ]
    }
   ],
   "source": [
    "evaluation('output_Char_cnn_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
