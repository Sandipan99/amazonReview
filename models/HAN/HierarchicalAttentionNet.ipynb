{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "from torch.nn.utils import rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(translator,stop_words,ps,**kwargs): # obtains a word to index mapping as well\n",
    "    count = 0\n",
    "    vocabulary = Counter()\n",
    "    w2i = {}\n",
    "    for label,fname in kwargs.items():\n",
    "        with open(fname+'_cleaned','w') as ft:\n",
    "            with open(fname) as fs:\n",
    "                for line in fs:\n",
    "                    count+=1\n",
    "                    label = line[0]\n",
    "                    review = line[2:]\n",
    "                    label = line[0]\n",
    "                    review = line[2:]\n",
    "                    sents = review.strip().split('.')\n",
    "                    sents = [[ps.stem(w) for w in word_tokenize(s.translate(translator)) if w not in stop_words] \\\n",
    "                             for s in sents if len(s)>1]\n",
    "                    words = sum(sents,[])\n",
    "                    for w in words:\n",
    "                        vocabulary[w] = 1\n",
    "                    rev = '.'.join([' '.join(s) for s in sents])\n",
    "                    ft.write(label+','+rev)\n",
    "                    ft.write('\\n')\n",
    "                    if count%1000000==0:  \n",
    "                        print('cleaned reviews: ',count)\n",
    "                        \n",
    "    count = 1\n",
    "    for w in vocabulary:\n",
    "        w2i[w]=count\n",
    "        count+=1\n",
    "    return w2i    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = str.maketrans('', '', string.punctuation)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "w2i = clean(translator,stop_words,ps,file='../Data/test_s.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "644"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(w2i)                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2tensor(review,w2i):\n",
    "    out = [[w2i[w] for w in sents.split()] for sents in review if len(sents)>0] \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creatingDataset(fname,w2i):  # dictionary of list of tuples (rev,label)\n",
    "    dataset = {}\n",
    "    with open(fname+'_cleaned') as fs:\n",
    "        for line in fs:\n",
    "            label = int(line[0])\n",
    "            review = line[2:]\n",
    "            temp = review.strip().split('.')\n",
    "            length = len(temp)\n",
    "            if length not in dataset:\n",
    "                dataset[length] = []\n",
    "            encoded_review = text2tensor(temp,w2i)\n",
    "            dataset[length].append((encoded_review,label))\n",
    "    return dataset    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = creatingDataset('../Data/test_s.csv',w2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 2\n",
      "3 13\n",
      "23 1\n",
      "2 4\n",
      "9 1\n",
      "5 3\n",
      "11 1\n"
     ]
    }
   ],
   "source": [
    "for key in train_dataset:\n",
    "    print(key,len(train_dataset[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([6, 3, 23, 2, 9, 5, 11])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createBatches(dataset,batch_size): # generator implementation \n",
    "    batch = [] # return a batch of datapoints based on batch_size\n",
    "    lengths = list(dataset.keys())\n",
    "    lengths.sort(reverse=True)\n",
    "    size = 0\n",
    "    sent_length = []\n",
    "    for l in lengths:\n",
    "        for doc in dataset[l]:\n",
    "            if len(batch)>0:\n",
    "                curr_len = len(batch[-1][0])\n",
    "            else:\n",
    "                curr_len = l\n",
    "            diff = curr_len - len(doc[0]) \n",
    "            if diff<=2 and diff>0:\n",
    "                size+=1\n",
    "                batch.append(doc)\n",
    "                sent_length.append(len(doc[0]))\n",
    "            elif diff==0:\n",
    "                batch.append(doc)\n",
    "                sent_length.append(len(doc[0]))\n",
    "                size+=1\n",
    "            else:\n",
    "                if len(batch)>0:\n",
    "                    yield (batch,sent_length)\n",
    "                batch = [doc]\n",
    "                sent_length = [len(doc[0])]\n",
    "                size = 1\n",
    "                \n",
    "            if size==batch_size:\n",
    "                yield (batch,sent_length)\n",
    "                batch = []\n",
    "                sent_length = []\n",
    "                size = 0\n",
    "    yield (batch,sent_length)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergeSentences(batch,lengths):\n",
    "    sent = []\n",
    "    label = []\n",
    "    for review,l in batch:\n",
    "        sent+=review\n",
    "        label.append(l)\n",
    "    return sent,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordEnc = wordEncoder(644,15,20,15,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentEnc = sentenceEncoder(15,20,15,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([22, 33, 20])\n",
      "tensor([21, 20, 15,  7, 13, 18,  0,  1,  2,  4,  3, 19, 10,  9, 17, 14,  5, 11,\n",
      "        16,  8, 12,  6])\n",
      "torch.Size([1, 22, 15])\n",
      "torch.Size([22, 15])\n",
      "tensor([[[0.4688, 0.4144],\n",
      "         [0.4674, 0.4158],\n",
      "         [0.4672, 0.4146],\n",
      "         [0.4668, 0.4159]]], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "for batch,lengths in createBatches(train_dataset,4):\n",
    "    if len(lengths)<3:\n",
    "        continue\n",
    "    sent,label = mergeSentences(batch,lengths)\n",
    "    sentence_length = [len(s) for s in sent]\n",
    "    sent = np.array(list(itertools.zip_longest(*sent, fillvalue=0))).T\n",
    "    X = torch.from_numpy(sent)\n",
    "    X_lengths = torch.LongTensor(sentence_length)\n",
    "    X,X_lengths,mapped_index = sortbylength(X,X_lengths)\n",
    "    batch_size = len(sentence_length)\n",
    "    \n",
    "    sent_out = wordEnc(X,X_lengths,batch_size)\n",
    "    sent_out = sent_out.squeeze()[mapped_index,:]\n",
    "    \n",
    "    curr_length = lengths[0]\n",
    "    \n",
    "    review_batch = torch.Tensor()\n",
    "    \n",
    "    r = 0\n",
    "    c = sent_out.shape[1]\n",
    "    for l in lengths:\n",
    "        if l==curr_length:\n",
    "            review_batch = torch.cat((review_batch,sent_out[r:r+l,:]))\n",
    "            r+=l\n",
    "        else:\n",
    "            diff = curr_length-l\n",
    "            review_batch = torch.cat((review_batch,sent_out[r:r+l,:],torch.zeros(diff,c)))\n",
    "            r+=l\n",
    "            \n",
    "    review_batch = review_batch.view(len(lengths),-1,c)\n",
    "    \n",
    "    output = sentEnc(review_batch,torch.LongTensor(lengths),len(lengths))\n",
    "    \n",
    "    print(output)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_out = sent_out.squeeze(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2259,  0.1208, -0.1167,  0.2175,  0.0648, -0.0127, -0.1858, -0.1479,\n",
       "         -0.2402,  0.0225,  0.1729, -0.0833,  0.0250, -0.1427, -0.0821],\n",
       "        [ 0.1890,  0.1215, -0.2516,  0.2067,  0.0165,  0.0355, -0.1486, -0.1060,\n",
       "         -0.3151,  0.0328,  0.1289, -0.0100,  0.0554, -0.0558,  0.0320],\n",
       "        [ 0.2342,  0.0826, -0.0822,  0.0912, -0.0436, -0.0420, -0.2195, -0.0939,\n",
       "         -0.3350,  0.0054,  0.1092,  0.0267,  0.0347, -0.0588,  0.1323],\n",
       "        [ 0.1675,  0.0539, -0.1533,  0.1286,  0.0015, -0.0280, -0.1306, -0.1284,\n",
       "         -0.2468,  0.1079,  0.0877,  0.0684,  0.0168,  0.0053,  0.0494],\n",
       "        [ 0.2044,  0.1118, -0.1821,  0.1809, -0.0049,  0.0298, -0.2043, -0.1616,\n",
       "         -0.2614,  0.0036,  0.1073, -0.0557,  0.0031, -0.0318, -0.0502]],\n",
       "       grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "sent = np.array(list(itertools.zip_longest(*sent, fillvalue=0))).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = torch.from_numpy(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 6, 5, 5]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_idx = 0\n",
    "embed = nn.Embedding(5, 10, padding_idx=padding_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9230,  0.6575, -1.4323, -0.8788,  0.8231,  0.5026,  0.8340,  1.1535,\n",
       "         -0.9093, -0.5471],\n",
       "        [-0.3955, -1.6489,  1.7715,  1.4298,  1.9349, -0.1394, -0.9060, -0.1989,\n",
       "         -0.5451,  0.9496]], grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.tensor([1,2],dtype=torch.long)\n",
    "embed(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "class wordEncoder(nn.Module):\n",
    "    def __init__(self,input_size,encoding_size,hidden_size,output_size,padding_idx):\n",
    "        super(wordEncoder,self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_first = True\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, encoding_size, padding_idx=padding_idx)\n",
    "        self.e2i = nn.Linear(encoding_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.h2o = nn.Linear(2*hidden_size, output_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.u_w = nn.Parameter(torch.rand(output_size)) # word context\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        \n",
    "    def forward(self, X, X_lengths, batch_size):\n",
    "        \n",
    "        self.hidden = self.initHidden(batch_size)\n",
    "        X = self.embedding(X)\n",
    "        X = self.e2i(X)\n",
    "\n",
    "        X = rnn.pack_padded_sequence(X, X_lengths, batch_first=True)\n",
    "\n",
    "        X, self.hidden = self.gru(X, self.hidden)\n",
    "\n",
    "        X, _ = torch.nn.utils.rnn.pad_packed_sequence(X, batch_first=True)\n",
    "\n",
    "        H = torch.unbind(X,dim=0)\n",
    "        X = self.h2o(X)\n",
    "        X = self.tanh(X)\n",
    "        \n",
    "        Y = torch.unbind(X,dim=0)\n",
    "        \n",
    "        Y_1 = torch.Tensor()\n",
    "        for i in range(X_lengths.shape[0]):\n",
    "            x = self.softmax(torch.sum(Y[i][:X_lengths[i].item()]*self.u_w,dim=1)).view(-1,1)\n",
    "            Y_1 = torch.cat((Y_1,torch.sum(H[i][:X_lengths[i].item()]*x,dim=0).view(1,1,-1)),dim=1)\n",
    "        \n",
    "        return Y_1\n",
    "        \n",
    "    def initHidden(self,batch_size):\n",
    "         return torch.zeros(2, batch_size, self.hidden_size)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.LongTensor(4,5).random_(1,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = torch.LongTensor([5,5,4,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_encoder = wordEncoder(10,15,20,15,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 5, 20])\n"
     ]
    }
   ],
   "source": [
    "out = word_encoder(inp,length,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0998, -0.1925,  0.1959, -0.2869, -0.0759, -0.3332,  0.0517,\n",
       "           0.2547,  0.2556,  0.0702,  0.2077, -0.2388,  0.0432,  0.2740,\n",
       "           0.1155],\n",
       "         [ 0.0053, -0.2452,  0.1693, -0.1561,  0.0096, -0.2666,  0.0875,\n",
       "           0.1585,  0.2412,  0.1068,  0.1697, -0.2195,  0.1377,  0.0993,\n",
       "           0.0125],\n",
       "         [ 0.0769, -0.0422,  0.1295, -0.1372,  0.0156, -0.3321,  0.0245,\n",
       "           0.2075,  0.1447,  0.1233,  0.1157, -0.2569,  0.2089,  0.0249,\n",
       "           0.0300],\n",
       "         [-0.1507, -0.2629,  0.1344, -0.1698, -0.0832, -0.1477,  0.1191,\n",
       "           0.1807,  0.1812,  0.0761,  0.1205, -0.2088, -0.0027,  0.1596,\n",
       "           0.0796]]], grad_fn=<CatBackward>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_1 = torch.LongTensor(3,5).random_(1,10)\n",
    "length_1 = torch.LongTensor([5,5,5]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_1 = word_encoder(inp_1,length_1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 15])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 15])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_1.squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.zeros(1,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 15])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_1 = torch.cat((out_1.squeeze(),z)).unsqueeze(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 15])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 15])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_s = torch.cat((out,out_1),dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 15])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 15])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 15])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sentenceEncoder(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,repr_size,output_size):\n",
    "        super(sentenceEncoder,self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.gru = nn.GRU(input_size, hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.g2r = nn.Linear(2*hidden_size,repr_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.u_s = nn.Parameter(torch.rand(repr_size)) # sentence context\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        self.r2o = nn.Linear(2*hidden_size,output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, X, X_lengths, batch_size):\n",
    "        self.hidden = self.initHidden(batch_size)\n",
    "\n",
    "        X = rnn.pack_padded_sequence(X, X_lengths, batch_first=True)\n",
    "\n",
    "        X, self.hidden = self.gru(X, self.hidden)\n",
    "\n",
    "        X, _ = torch.nn.utils.rnn.pad_packed_sequence(X, batch_first=True)\n",
    "\n",
    "        H = torch.unbind(X,dim=0) # hidden state obtained from each sentence    \n",
    "        \n",
    "        X = self.g2r(X)\n",
    "        X = self.tanh(X)\n",
    "        \n",
    "        Y = torch.unbind(X,dim=0)\n",
    "        \n",
    "        Y_1 = torch.Tensor()\n",
    "        for i in range(X_lengths.shape[0]):\n",
    "            x = self.softmax(torch.sum(Y[i][:X_lengths[i].item()]*self.u_s,dim=1)).view(-1,1)\n",
    "            Y_1 = torch.cat((Y_1,torch.sum(H[i][:X_lengths[i].item()]*x,dim=0).view(1,1,-1)),dim=1)\n",
    "        \n",
    "        output = self.r2o(Y_1)\n",
    "        output = self.sigmoid(output)\n",
    "        \n",
    "        return output\n",
    "        \n",
    "    def initHidden(self,batch_size):\n",
    "         return torch.zeros(2, batch_size, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_enc = sentenceEncoder(15,20,15,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_s_length = torch.LongTensor([4,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5318, 0.5057],\n",
       "         [0.5416, 0.5014]]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_enc(input_s,inp_s_length,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inp (4,5), inp_1 (3,5), length (5,5,4,3), length_1 (5,5,5), \n",
    "len_sent = torch.LongTensor([4,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sent = torch.cat((inp,inp_1),dim=0)\n",
    "all_sent_len = torch.cat((length,length_1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sent[2,4] = 0\n",
    "all_sent[3,4] = 0\n",
    "all_sent[3,3] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3, 3, 8, 3, 2],\n",
       "        [8, 7, 7, 5, 8],\n",
       "        [9, 7, 6, 9, 0],\n",
       "        [1, 1, 8, 0, 0],\n",
       "        [8, 2, 2, 4, 3],\n",
       "        [7, 5, 1, 3, 8],\n",
       "        [1, 7, 6, 7, 3]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 5, 4, 3, 5, 5, 5])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_sent_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.LongTensor([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenating sentences from all reviews in a batch and then sorting them based on length would change the order\n",
    "# to keep track of the sequence of the sentences we need to remember the original mapping\n",
    "# this routine keeps track of the sequence of the sentences among all the reviews\n",
    "# need when passing to the sentence encoder...\n",
    "\n",
    "def originalMap(indices):  \n",
    "    count = 0\n",
    "    m = {}\n",
    "    for i in indices:\n",
    "        m[i.item()] = count\n",
    "        count+=1\n",
    "\n",
    "    m_p = []    \n",
    "    for i,val in sorted(m.items(),key=lambda x:x[0]):\n",
    "        m_p.append(val)\n",
    "\n",
    "    return torch.LongTensor(m_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sortbylength(all_sent,all_sent_len):\n",
    "    sorted_lengths, indices = torch.sort(all_sent_len,descending=True)\n",
    "    mapped_index = originalMap(indices)\n",
    "    return all_sent[torch.LongTensor(indices),:],sorted_lengths,mapped_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,X_lengths,mapped_index = sortbylength(all_sent,all_sent_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = (all_sent,all_sent_len,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 5])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_sent.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordEnc = wordEncoder(644,15,20,15,0)\n",
    "sentEnc = sentenceEncoder(40,20,15,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(wordEnc,sentEnc,train_dataset,batch_size=4,epochs=1,learning_rate=0.001):\n",
    "    \n",
    "    wordEnc_optimizer = optim.Adam(wordEnc.parameters(), lr=learning_rate)\n",
    "    sentEnc_optimizer = optim.Adam(sentEnc.parameters(), lr=learning_rate)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for _ in range(epochs):\n",
    "        for batch,lengths in createBatches(train_dataset,4):\n",
    "            if len(lengths)<3:\n",
    "                continue\n",
    "            sent,label = mergeSentences(batch,lengths)\n",
    "            label = torch.LongTensor(label)\n",
    "            print(label)\n",
    "            sentence_length = [len(s) for s in sent]\n",
    "            sent = np.array(list(itertools.zip_longest(*sent, fillvalue=0))).T\n",
    "            X = torch.from_numpy(sent)\n",
    "            X_lengths = torch.LongTensor(sentence_length)\n",
    "            X,X_lengths,mapped_index = sortbylength(X,X_lengths)\n",
    "            batch_size = len(sentence_length)\n",
    "\n",
    "            sent_out = wordEnc(X,X_lengths,batch_size)\n",
    "            print('output from word encoder obtained')\n",
    "            print(sent_out.shape)\n",
    "            \n",
    "            sent_out = sent_out.squeeze()[mapped_index,:]\n",
    "\n",
    "            curr_length = lengths[0]\n",
    "\n",
    "            review_batch = torch.Tensor()\n",
    "\n",
    "            r = 0\n",
    "            c = sent_out.shape[1]\n",
    "            for l in lengths:\n",
    "                if l==curr_length:\n",
    "                    review_batch = torch.cat((review_batch,sent_out[r:r+l,:]))\n",
    "                    r+=l\n",
    "                else:\n",
    "                    diff = curr_length-l\n",
    "                    review_batch = torch.cat((review_batch,sent_out[r:r+l,:],torch.zeros(diff,c)))\n",
    "                    r+=l\n",
    "\n",
    "            review_batch = review_batch.view(len(lengths),-1,c)\n",
    "\n",
    "            print('review batch')\n",
    "            print(review_batch.shape)\n",
    "            output = sentEnc(review_batch,torch.LongTensor(lengths),len(lengths))\n",
    "            \n",
    "            print(output.shape)\n",
    "            \n",
    "            loss = criterion(output.squeeze(),label)\n",
    "            \n",
    "            print(loss)\n",
    "            wordEnc_optimizer.zero_grad()\n",
    "            sentEnc_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            sentEnc_optimizer.step()\n",
    "            wordEnc_optimizer.step()\n",
    "            break\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 0, 0])\n",
      "hidden_size\n",
      "torch.Size([33, 40])\n",
      "output from word encoder obtained\n",
      "torch.Size([1, 22, 40])\n",
      "review batch\n",
      "torch.Size([4, 6, 40])\n",
      "output from GRU obtained\n",
      "size of hidden state\n",
      "torch.Size([6, 40])\n",
      "linear and tanh transformation done\n",
      "torch.Size([4, 6, 15])\n",
      "torch.Size([1, 4, 2])\n",
      "tensor(0.6865, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "train(wordEnc,sentEnc,train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordEnc = wordEncoder(10,15,20,15,0)\n",
    "sentEnc = sentenceEncoder(15,20,15,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,X_lengths,mapped_index = sortbylength(data_train[0],data_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 5, 20])\n"
     ]
    }
   ],
   "source": [
    "out = wordEnc(X,X_lengths,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 15])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_1 = torch.cat((out.squeeze(),torch.zeros(1,15))).unsqueeze(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 15])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_1 = out_1.view(2,-1,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 15])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5392, 0.5036],\n",
       "         [0.5387, 0.5031]]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_enc(out_1,torch.LongTensor([4,3]),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 5, 20])\n",
      "tensor(0.6927, grad_fn=<NllLoss2DBackward>)\n"
     ]
    }
   ],
   "source": [
    "train(wordEnc,sentEnc,data_train)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.array([1,2,3,5])"
=======
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
>>>>>>> 2ce92b144fced992b22fee923db3d7714fcc1a31
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 297,
=======
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('word2index.pickle','rb') as fs:\n",
    "    w2i = pickle.load(fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creatingDataset(fname,w2i):  # dictionary of list of tuples (rev,label)\n",
    "    dataset = {}\n",
    "    with open(fname+'_cleaned') as fs:\n",
    "        for line in fs:\n",
    "            label = int(line[0])\n",
    "            review = line[2:]\n",
    "            temp = review.strip().split('.')\n",
    "            encoded_review = text2tensor(temp,w2i)\n",
    "            length = len(encoded_review)\n",
    "            if length not in dataset and length>0:\n",
    "                dataset[length] = []\n",
    "            if len(encoded_review)>0:\n",
    "                dataset[length].append((encoded_review,label))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2tensor(review,w2i):\n",
    "    out = [[w2i[w] for w in sents.split()] for sents in review if len(sents)>0]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = creatingDataset('../Data/train.csv', w2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createBatches(dataset, batch_size):  # generator implementation\n",
    "    batch = []  # return a batch of datapoints based on batch_size\n",
    "    lengths = list(dataset.keys())\n",
    "    lengths.sort()\n",
    "    size = 0\n",
    "    sent_length = []\n",
    "    \n",
    "    for l in lengths[:100]:\n",
    "        for doc in dataset[l]:\n",
    "            if len(doc[0])==l:\n",
    "                batch.append(doc)\n",
    "                sent_length.append(len(doc[0]))\n",
    "                size+=1\n",
    "            if size==batch_size:\n",
    "                yield(batch,sent_length)\n",
    "                batch = []\n",
    "                sent_length = []\n",
    "                size = 0\n",
    "                \n",
    "        yield(batch,sent_length)\n",
    "        batch = []\n",
    "        sent_length = []\n",
    "        size = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of sizes:\n",
    "keys = list(train_dataset.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2899549\n",
      "2 5740726\n",
      "3 5897990\n",
      "4 3913050\n",
      "5 2479805\n",
      "6 1669319\n",
      "7 1184796\n",
      "8 872828\n",
      "9 663894\n",
      "10 518487\n",
      "11 416298\n",
      "12 337900\n",
      "13 278558\n",
      "14 232642\n",
      "15 195719\n",
      "16 166574\n",
      "17 142636\n",
      "18 122986\n",
      "19 107156\n",
      "20 93563\n",
      "21 81876\n",
      "22 72085\n",
      "23 63604\n",
      "24 56658\n",
      "25 49760\n",
      "26 43882\n",
      "27 39101\n",
      "28 35149\n",
      "29 31613\n",
      "30 27989\n",
      "31 25111\n",
      "32 22792\n",
      "33 20464\n",
      "34 18696\n",
      "35 16859\n",
      "36 15446\n",
      "37 13678\n",
      "38 12562\n",
      "39 11543\n",
      "40 10479\n",
      "41 9574\n",
      "42 8890\n",
      "43 8033\n",
      "44 7472\n",
      "45 6873\n",
      "46 6376\n",
      "47 5855\n",
      "48 5365\n",
      "49 4987\n",
      "50 4649\n",
      "51 4212\n",
      "52 3897\n",
      "53 3599\n",
      "54 3276\n",
      "55 3075\n",
      "56 2830\n",
      "57 2672\n",
      "58 2459\n",
      "59 2258\n",
      "60 2105\n",
      "61 1957\n",
      "62 1832\n",
      "63 1737\n",
      "64 1633\n",
      "65 1612\n",
      "66 1423\n",
      "67 1276\n",
      "68 1284\n",
      "69 1191\n",
      "70 1057\n",
      "71 1071\n",
      "72 989\n",
      "73 917\n",
      "74 905\n",
      "75 815\n",
      "76 748\n",
      "77 770\n",
      "78 647\n",
      "79 634\n",
      "80 609\n",
      "81 566\n",
      "82 574\n",
      "83 503\n",
      "84 472\n",
      "85 449\n",
      "86 404\n",
      "87 387\n",
      "88 371\n",
      "89 387\n",
      "90 378\n",
      "91 357\n",
      "92 311\n",
      "93 315\n",
      "94 270\n",
      "95 268\n",
      "96 280\n",
      "97 243\n",
      "98 242\n",
      "99 220\n",
      "100 196\n",
      "101 196\n",
      "102 173\n",
      "103 211\n",
      "104 184\n",
      "105 159\n",
      "106 157\n",
      "107 196\n",
      "108 149\n",
      "109 130\n",
      "110 114\n",
      "111 152\n",
      "112 131\n",
      "113 103\n",
      "114 115\n",
      "115 94\n",
      "116 116\n",
      "117 117\n",
      "118 100\n",
      "119 101\n",
      "120 98\n",
      "121 85\n",
      "122 79\n",
      "123 87\n",
      "124 104\n",
      "125 76\n",
      "126 63\n",
      "127 84\n",
      "128 66\n",
      "129 70\n",
      "130 47\n",
      "131 48\n",
      "132 59\n",
      "133 52\n",
      "134 53\n",
      "135 54\n",
      "136 57\n",
      "137 64\n",
      "138 45\n",
      "139 44\n",
      "140 56\n",
      "141 43\n",
      "142 49\n",
      "143 38\n",
      "144 35\n",
      "145 37\n",
      "146 33\n",
      "147 37\n",
      "148 44\n",
      "149 45\n",
      "150 25\n",
      "151 38\n",
      "152 29\n",
      "153 33\n",
      "154 29\n",
      "155 47\n",
      "156 29\n",
      "157 23\n",
      "158 20\n",
      "159 28\n",
      "160 27\n",
      "161 25\n",
      "162 34\n",
      "163 24\n",
      "164 22\n",
      "165 18\n",
      "166 24\n",
      "167 19\n",
      "168 18\n",
      "169 25\n",
      "170 21\n",
      "171 19\n",
      "172 19\n",
      "173 23\n",
      "174 12\n",
      "175 16\n",
      "176 16\n",
      "177 17\n",
      "178 11\n",
      "179 14\n",
      "180 17\n",
      "181 17\n",
      "182 16\n",
      "183 11\n",
      "184 18\n",
      "185 14\n",
      "186 10\n",
      "187 9\n",
      "188 10\n",
      "189 6\n",
      "190 17\n",
      "191 6\n",
      "192 21\n",
      "193 14\n",
      "194 8\n",
      "195 9\n",
      "196 7\n",
      "197 14\n",
      "198 5\n",
      "199 10\n",
      "200 11\n",
      "201 9\n",
      "202 11\n",
      "203 7\n",
      "204 7\n",
      "205 8\n",
      "206 10\n",
      "207 6\n",
      "208 11\n",
      "209 8\n",
      "210 6\n",
      "211 9\n",
      "212 5\n",
      "213 10\n",
      "214 10\n",
      "215 9\n",
      "216 11\n",
      "217 5\n",
      "218 6\n",
      "219 6\n",
      "220 5\n",
      "221 9\n",
      "222 5\n",
      "223 8\n",
      "224 4\n",
      "225 10\n",
      "226 4\n",
      "227 2\n",
      "228 6\n",
      "229 3\n",
      "230 7\n",
      "231 5\n",
      "232 13\n",
      "233 10\n",
      "234 7\n",
      "235 3\n",
      "236 1\n",
      "237 2\n",
      "238 5\n",
      "239 4\n",
      "240 5\n",
      "241 5\n",
      "242 4\n",
      "243 2\n",
      "244 6\n",
      "245 6\n",
      "246 4\n",
      "247 4\n",
      "248 9\n",
      "249 4\n",
      "250 6\n",
      "251 2\n",
      "252 2\n",
      "253 7\n",
      "255 6\n",
      "256 1\n",
      "257 3\n",
      "258 3\n",
      "259 4\n",
      "260 6\n",
      "261 3\n",
      "263 2\n",
      "264 5\n",
      "265 6\n",
      "266 1\n",
      "267 6\n",
      "268 1\n",
      "269 6\n",
      "270 2\n",
      "271 2\n",
      "272 1\n",
      "273 5\n",
      "274 1\n",
      "275 2\n",
      "276 3\n",
      "277 4\n",
      "278 2\n",
      "279 3\n",
      "280 1\n",
      "281 2\n",
      "282 1\n",
      "283 2\n",
      "284 1\n",
      "285 3\n",
      "286 1\n",
      "287 5\n",
      "288 5\n",
      "289 1\n",
      "290 1\n",
      "291 2\n",
      "292 2\n",
      "294 2\n",
      "296 3\n",
      "297 1\n",
      "298 2\n",
      "299 3\n",
      "300 3\n",
      "302 3\n",
      "303 1\n",
      "304 2\n",
      "305 1\n",
      "306 2\n",
      "307 4\n",
      "309 1\n",
      "311 2\n",
      "312 1\n",
      "314 1\n",
      "316 1\n",
      "317 1\n",
      "320 2\n",
      "323 2\n",
      "325 1\n",
      "329 1\n",
      "332 2\n",
      "333 1\n",
      "337 1\n",
      "340 1\n",
      "342 3\n",
      "344 2\n",
      "347 1\n",
      "348 1\n",
      "350 1\n",
      "351 1\n",
      "352 2\n",
      "364 1\n",
      "369 1\n",
      "371 1\n",
      "372 1\n",
      "376 1\n",
      "378 1\n",
      "384 5\n",
      "385 1\n",
      "387 1\n",
      "389 1\n",
      "394 2\n",
      "397 1\n",
      "400 1\n",
      "402 1\n",
      "408 1\n",
      "412 1\n",
      "431 2\n",
      "438 1\n",
      "444 1\n",
      "446 1\n",
      "462 1\n",
      "468 1\n",
      "478 1\n",
      "505 1\n",
      "508 1\n",
      "513 1\n",
      "597 1\n",
      "701 1\n",
      "840 1\n"
     ]
    }
   ],
   "source": [
    "reviews = []\n",
    "for k in keys:\n",
    "    print(k,len(train_dataset[k]))\n",
    "    reviews.append(len(train_dataset[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2930085\n",
      "2 5810754\n",
      "3 5954512\n",
      "4 3933658\n",
      "5 2478662\n",
      "6 1658461\n",
      "7 1169994\n",
      "8 858229\n",
      "9 649140\n",
      "10 506339\n",
      "11 404496\n",
      "12 327963\n",
      "13 269831\n",
      "14 224321\n",
      "15 189079\n",
      "16 160086\n",
      "17 136706\n",
      "18 118658\n",
      "19 102652\n",
      "20 89349\n",
      "21 78390\n",
      "22 68800\n",
      "23 60531\n",
      "24 54001\n",
      "25 47235\n",
      "26 41861\n",
      "27 37308\n",
      "28 33520\n",
      "29 29877\n",
      "30 26407\n",
      "31 24109\n",
      "32 21586\n",
      "33 19519\n",
      "34 17770\n",
      "35 16023\n",
      "36 14451\n",
      "37 12994\n",
      "38 12061\n",
      "39 10857\n",
      "40 10090\n",
      "41 9087\n",
      "42 8543\n",
      "43 7548\n",
      "44 7151\n",
      "45 6552\n",
      "46 6045\n",
      "47 5497\n",
      "48 5097\n",
      "49 4726\n",
      "50 4470\n",
      "51 3996\n",
      "52 3633\n",
      "53 3448\n",
      "54 3115\n",
      "55 2983\n",
      "56 2698\n",
      "57 2457\n",
      "58 2376\n",
      "59 2066\n",
      "60 2056\n",
      "61 1818\n",
      "62 1817\n",
      "63 1695\n",
      "64 1595\n",
      "65 1384\n",
      "66 1372\n",
      "67 1249\n",
      "68 1264\n",
      "69 1074\n",
      "70 1045\n",
      "71 1017\n",
      "72 951\n",
      "73 893\n",
      "74 780\n",
      "75 810\n",
      "76 755\n",
      "77 706\n",
      "78 600\n",
      "79 591\n",
      "80 594\n",
      "81 573\n",
      "82 530\n",
      "83 472\n",
      "84 447\n",
      "85 421\n",
      "86 371\n",
      "87 394\n",
      "88 386\n",
      "89 368\n",
      "90 357\n",
      "91 303\n",
      "92 317\n",
      "93 291\n",
      "94 269\n",
      "95 274\n",
      "96 233\n",
      "97 229\n",
      "98 235\n",
      "99 218\n",
      "100 162\n",
      "101 197\n",
      "102 205\n",
      "103 207\n",
      "104 157\n",
      "105 146\n",
      "106 181\n",
      "107 161\n",
      "108 138\n",
      "109 119\n",
      "110 124\n",
      "111 125\n",
      "112 118\n",
      "113 113\n",
      "114 110\n",
      "115 102\n",
      "116 114\n",
      "117 106\n",
      "118 98\n",
      "119 103\n",
      "120 92\n",
      "121 81\n",
      "122 71\n",
      "123 93\n",
      "124 89\n",
      "125 76\n",
      "126 70\n",
      "127 73\n",
      "128 71\n",
      "129 50\n",
      "130 51\n",
      "131 46\n",
      "132 60\n",
      "133 49\n",
      "134 54\n",
      "135 63\n",
      "136 61\n",
      "137 51\n",
      "138 40\n",
      "139 52\n",
      "140 43\n",
      "141 46\n",
      "142 31\n",
      "143 37\n",
      "144 36\n",
      "145 37\n",
      "146 39\n",
      "147 43\n",
      "148 43\n",
      "149 40\n",
      "150 26\n",
      "151 28\n",
      "152 34\n",
      "153 32\n",
      "154 41\n",
      "155 31\n",
      "156 27\n",
      "157 24\n",
      "158 15\n",
      "159 28\n",
      "160 32\n",
      "161 28\n",
      "162 29\n",
      "163 20\n",
      "164 24\n",
      "165 21\n",
      "166 18\n",
      "167 17\n",
      "168 18\n",
      "169 21\n",
      "170 20\n",
      "171 23\n",
      "172 20\n",
      "173 9\n",
      "174 22\n",
      "175 16\n",
      "176 13\n",
      "177 7\n",
      "178 19\n",
      "179 18\n",
      "180 15\n",
      "181 11\n",
      "182 13\n",
      "183 19\n",
      "184 21\n",
      "185 8\n",
      "186 9\n",
      "187 14\n",
      "188 7\n",
      "189 11\n",
      "190 11\n",
      "191 13\n",
      "192 12\n",
      "193 9\n",
      "194 13\n",
      "195 10\n",
      "196 8\n",
      "197 5\n",
      "198 12\n",
      "199 10\n",
      "200 7\n",
      "201 16\n",
      "202 7\n",
      "203 8\n",
      "204 7\n",
      "205 8\n",
      "206 3\n",
      "207 15\n",
      "208 8\n",
      "209 4\n",
      "210 9\n",
      "211 6\n",
      "212 9\n",
      "213 11\n",
      "214 8\n",
      "215 10\n",
      "216 8\n",
      "217 3\n",
      "218 8\n",
      "219 7\n",
      "220 6\n",
      "221 5\n",
      "222 6\n",
      "223 6\n",
      "224 5\n",
      "225 6\n",
      "226 6\n",
      "227 5\n",
      "228 7\n",
      "229 7\n",
      "230 3\n",
      "231 14\n",
      "232 7\n",
      "233 6\n",
      "234 2\n",
      "235 1\n",
      "236 2\n",
      "237 3\n",
      "238 9\n",
      "239 6\n",
      "240 4\n",
      "241 2\n",
      "242 5\n",
      "243 3\n",
      "244 2\n",
      "245 7\n",
      "246 5\n",
      "247 5\n",
      "248 7\n",
      "249 6\n",
      "251 3\n",
      "252 3\n",
      "253 4\n",
      "254 3\n",
      "255 6\n",
      "256 1\n",
      "257 7\n",
      "258 3\n",
      "259 4\n",
      "260 3\n",
      "262 2\n",
      "263 4\n",
      "264 2\n",
      "265 4\n",
      "266 6\n",
      "267 2\n",
      "268 6\n",
      "269 1\n",
      "270 2\n",
      "271 3\n",
      "272 2\n",
      "275 4\n",
      "276 6\n",
      "277 3\n",
      "278 1\n",
      "279 2\n",
      "280 1\n",
      "281 2\n",
      "283 3\n",
      "284 1\n",
      "285 1\n",
      "286 2\n",
      "287 7\n",
      "288 3\n",
      "290 3\n",
      "291 1\n",
      "294 3\n",
      "295 2\n",
      "296 4\n",
      "297 2\n",
      "298 2\n",
      "299 1\n",
      "300 2\n",
      "301 2\n",
      "302 1\n",
      "303 1\n",
      "304 2\n",
      "305 1\n",
      "306 2\n",
      "307 1\n",
      "308 1\n",
      "310 1\n",
      "311 2\n",
      "314 2\n",
      "315 1\n",
      "317 1\n",
      "320 1\n",
      "321 1\n",
      "322 1\n",
      "324 1\n",
      "327 1\n",
      "331 1\n",
      "332 2\n",
      "334 1\n",
      "337 1\n",
      "338 1\n",
      "340 1\n",
      "341 2\n",
      "342 1\n",
      "343 2\n",
      "344 1\n",
      "347 1\n",
      "351 2\n",
      "359 1\n",
      "366 1\n",
      "367 1\n",
      "369 1\n",
      "372 1\n",
      "374 1\n",
      "382 1\n",
      "383 5\n",
      "387 1\n",
      "388 1\n",
      "393 1\n",
      "394 2\n",
      "399 1\n",
      "402 1\n",
      "408 1\n",
      "410 1\n",
      "428 2\n",
      "436 1\n",
      "439 1\n",
      "446 1\n",
      "461 1\n",
      "465 1\n",
      "477 1\n",
      "494 1\n",
      "499 1\n",
      "513 1\n",
      "592 1\n",
      "682 1\n",
      "839 1\n"
     ]
    }
   ],
   "source": [
    "reviews = []\n",
    "for k in keys:\n",
    "    print(k,len(train_dataset[k]))\n",
    "    reviews.append(len(train_dataset[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28738514"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no problem in the batches\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for batch,lengths in createBatches(train_dataset,128):\n",
    "    count+=1\n",
    "    l = set(lengths)\n",
    "    if len(l)>1 or len(l)==0:\n",
    "        print(count)\n",
    "        break\n",
    "print('no problem in the batches')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
>>>>>>> 2ce92b144fced992b22fee923db3d7714fcc1a31
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "[1, 2, 3, 5]"
      ]
     },
     "execution_count": 297,
=======
       "([[197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 5, 93, 208, 209],\n",
       "  [210],\n",
       "  [93, 211, 212, 37],\n",
       "  [5, 88, 213, 214, 215],\n",
       "  [216, 180]],\n",
       " 1)"
      ]
     },
     "execution_count": 18,
>>>>>>> 2ce92b144fced992b22fee923db3d7714fcc1a31
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
<<<<<<< HEAD
    "n.tolist()"
=======
    "train_dataset[5][0]"
>>>>>>> 2ce92b144fced992b22fee923db3d7714fcc1a31
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
