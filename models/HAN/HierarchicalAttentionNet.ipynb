{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "from torch.nn.utils import rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(translator,stop_words,ps,**kwargs): # obtains a word to index mapping as well\n",
    "    count = 0\n",
    "    vocabulary = Counter()\n",
    "    w2i = {}\n",
    "    for label,fname in kwargs.items():\n",
    "        with open(fname+'_cleaned','w') as ft:\n",
    "            with open(fname) as fs:\n",
    "                for line in fs:\n",
    "                    count+=1\n",
    "                    label = line[0]\n",
    "                    review = line[2:]\n",
    "                    label = line[0]\n",
    "                    review = line[2:]\n",
    "                    sents = review.strip().split('.')\n",
    "                    sents = [[ps.stem(w) for w in word_tokenize(s.translate(translator)) if w not in stop_words] \\\n",
    "                             for s in sents if len(s)>1]\n",
    "                    words = sum(sents,[])\n",
    "                    for w in words:\n",
    "                        vocabulary[w] = 1\n",
    "                    rev = '.'.join([' '.join(s) for s in sents])\n",
    "                    ft.write(label+','+rev)\n",
    "                    ft.write('\\n')\n",
    "                    if count%1000000==0:  \n",
    "                        print('cleaned reviews: ',count)\n",
    "                        \n",
    "    count = 1\n",
    "    for w in vocabulary:\n",
    "        w2i[w]=count\n",
    "        count+=1\n",
    "    return w2i    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = str.maketrans('', '', string.punctuation)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "w2i = clean(translator,stop_words,ps,file='../Data/test_s.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "644"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(w2i)                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2tensor(review,w2i):\n",
    "    out = [[w2i[w] for w in sents.split()] for sents in review if len(sents)>0] \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creatingDataset(fname,w2i):  # dictionary of list of tuples (rev,label)\n",
    "    dataset = {}\n",
    "    with open(fname+'_cleaned') as fs:\n",
    "        for line in fs:\n",
    "            label = int(line[0])\n",
    "            review = line[2:]\n",
    "            temp = review.strip().split('.')\n",
    "            length = len(temp)\n",
    "            if length not in dataset:\n",
    "                dataset[length] = []\n",
    "            encoded_review = text2tensor(temp,w2i)\n",
    "            dataset[length].append((encoded_review,label))\n",
    "    return dataset    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = creatingDataset('../Data/test_s.csv',w2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 2\n",
      "3 13\n",
      "23 1\n",
      "2 4\n",
      "9 1\n",
      "5 3\n",
      "11 1\n"
     ]
    }
   ],
   "source": [
    "for key in train_dataset:\n",
    "    print(key,len(train_dataset[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([6, 3, 23, 2, 9, 5, 11])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createBatches(dataset,batch_size): # generator implementation \n",
    "    batch = [] # return a batch of datapoints based on batch_size\n",
    "    lengths = list(dataset.keys())\n",
    "    lengths.sort(reverse=True)\n",
    "    size = 0\n",
    "    sent_length = []\n",
    "    for l in lengths:\n",
    "        for doc in dataset[l]:\n",
    "            if len(batch)>0:\n",
    "                curr_len = len(batch[-1][0])\n",
    "            else:\n",
    "                curr_len = l\n",
    "            diff = curr_len - len(doc[0]) \n",
    "            if diff<=2 and diff>0:\n",
    "                size+=1\n",
    "                batch.append(doc)\n",
    "                sent_length.append(len(doc[0]))\n",
    "            elif diff==0:\n",
    "                batch.append(doc)\n",
    "                sent_length.append(len(doc[0]))\n",
    "                size+=1\n",
    "            else:\n",
    "                if len(batch)>0:\n",
    "                    yield (batch,sent_length)\n",
    "                batch = [doc]\n",
    "                sent_length = [len(doc[0])]\n",
    "                size = 1\n",
    "                \n",
    "            if size==batch_size:\n",
    "                yield (batch,sent_length)\n",
    "                batch = []\n",
    "                sent_length = []\n",
    "                size = 0\n",
    "    yield (batch,sent_length)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergeSentences(batch,lengths):\n",
    "    sent = []\n",
    "    label = []\n",
    "    for review,l in batch:\n",
    "        sent+=review\n",
    "        label.append(l)\n",
    "    return sent,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordEnc = wordEncoder(644,15,20,15,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentEnc = sentenceEncoder(15,20,15,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([22, 33, 20])\n",
      "tensor([21, 20, 15,  7, 13, 18,  0,  1,  2,  4,  3, 19, 10,  9, 17, 14,  5, 11,\n",
      "        16,  8, 12,  6])\n",
      "torch.Size([1, 22, 15])\n",
      "torch.Size([22, 15])\n",
      "tensor([[[0.4688, 0.4144],\n",
      "         [0.4674, 0.4158],\n",
      "         [0.4672, 0.4146],\n",
      "         [0.4668, 0.4159]]], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "for batch,lengths in createBatches(train_dataset,4):\n",
    "    if len(lengths)<3:\n",
    "        continue\n",
    "    sent,label = mergeSentences(batch,lengths)\n",
    "    sentence_length = [len(s) for s in sent]\n",
    "    sent = np.array(list(itertools.zip_longest(*sent, fillvalue=0))).T\n",
    "    X = torch.from_numpy(sent)\n",
    "    X_lengths = torch.LongTensor(sentence_length)\n",
    "    X,X_lengths,mapped_index = sortbylength(X,X_lengths)\n",
    "    batch_size = len(sentence_length)\n",
    "    \n",
    "    sent_out = wordEnc(X,X_lengths,batch_size)\n",
    "    sent_out = sent_out.squeeze()[mapped_index,:]\n",
    "    \n",
    "    curr_length = lengths[0]\n",
    "    \n",
    "    review_batch = torch.Tensor()\n",
    "    \n",
    "    r = 0\n",
    "    c = sent_out.shape[1]\n",
    "    for l in lengths:\n",
    "        if l==curr_length:\n",
    "            review_batch = torch.cat((review_batch,sent_out[r:r+l,:]))\n",
    "            r+=l\n",
    "        else:\n",
    "            diff = curr_length-l\n",
    "            review_batch = torch.cat((review_batch,sent_out[r:r+l,:],torch.zeros(diff,c)))\n",
    "            r+=l\n",
    "            \n",
    "    review_batch = review_batch.view(len(lengths),-1,c)\n",
    "    \n",
    "    output = sentEnc(review_batch,torch.LongTensor(lengths),len(lengths))\n",
    "    \n",
    "    print(output)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_out = sent_out.squeeze(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2259,  0.1208, -0.1167,  0.2175,  0.0648, -0.0127, -0.1858, -0.1479,\n",
       "         -0.2402,  0.0225,  0.1729, -0.0833,  0.0250, -0.1427, -0.0821],\n",
       "        [ 0.1890,  0.1215, -0.2516,  0.2067,  0.0165,  0.0355, -0.1486, -0.1060,\n",
       "         -0.3151,  0.0328,  0.1289, -0.0100,  0.0554, -0.0558,  0.0320],\n",
       "        [ 0.2342,  0.0826, -0.0822,  0.0912, -0.0436, -0.0420, -0.2195, -0.0939,\n",
       "         -0.3350,  0.0054,  0.1092,  0.0267,  0.0347, -0.0588,  0.1323],\n",
       "        [ 0.1675,  0.0539, -0.1533,  0.1286,  0.0015, -0.0280, -0.1306, -0.1284,\n",
       "         -0.2468,  0.1079,  0.0877,  0.0684,  0.0168,  0.0053,  0.0494],\n",
       "        [ 0.2044,  0.1118, -0.1821,  0.1809, -0.0049,  0.0298, -0.2043, -0.1616,\n",
       "         -0.2614,  0.0036,  0.1073, -0.0557,  0.0031, -0.0318, -0.0502]],\n",
       "       grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "sent = np.array(list(itertools.zip_longest(*sent, fillvalue=0))).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = torch.from_numpy(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 6, 5, 5]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_idx = 0\n",
    "embed = nn.Embedding(5, 10, padding_idx=padding_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9230,  0.6575, -1.4323, -0.8788,  0.8231,  0.5026,  0.8340,  1.1535,\n",
       "         -0.9093, -0.5471],\n",
       "        [-0.3955, -1.6489,  1.7715,  1.4298,  1.9349, -0.1394, -0.9060, -0.1989,\n",
       "         -0.5451,  0.9496]], grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.tensor([1,2],dtype=torch.long)\n",
    "embed(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class wordEncoder(nn.Module):\n",
    "    def __init__(self,input_size,encoding_size,hidden_size,output_size,padding_idx):\n",
    "        super(wordEncoder,self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_first = True\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, encoding_size, padding_idx=padding_idx)\n",
    "        self.e2i = nn.Linear(encoding_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.h2o = nn.Linear(2*hidden_size, output_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.u_w = nn.Parameter(torch.rand(output_size)) # word context\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        \n",
    "    def forward(self, X, X_lengths, batch_size):\n",
    "        \n",
    "        self.hidden = self.initHidden(batch_size)\n",
    "        X = self.embedding(X)\n",
    "        X = self.e2i(X)\n",
    "\n",
    "        X = rnn.pack_padded_sequence(X, X_lengths, batch_first=True)\n",
    "\n",
    "        X, self.hidden = self.gru(X, self.hidden)\n",
    "\n",
    "        X, _ = torch.nn.utils.rnn.pad_packed_sequence(X, batch_first=True)\n",
    "\n",
    "        X = self.h2o(X)\n",
    "        X = self.tanh(X)\n",
    "        \n",
    "        Y = torch.unbind(X,dim=0)\n",
    "        \n",
    "        Y_1 = torch.Tensor()\n",
    "        for i in range(X_lengths.shape[0]):\n",
    "            x = self.softmax(torch.sum(Y[i][:X_lengths[i].item()]*self.u_w,dim=1)).view(-1,1)\n",
    "            Y_1 = torch.cat((Y_1,torch.sum(Y[i][:X_lengths[i].item()]*x,dim=0).view(1,1,-1)),dim=1)\n",
    "        \n",
    "        return Y_1\n",
    "        \n",
    "    def initHidden(self,batch_size):\n",
    "         return torch.zeros(2, batch_size, self.hidden_size)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.LongTensor(4,5).random_(1,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = torch.LongTensor([5,5,4,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_encoder = wordEncoder(10,15,20,15,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 5, 20])\n"
     ]
    }
   ],
   "source": [
    "out = word_encoder(inp,length,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0998, -0.1925,  0.1959, -0.2869, -0.0759, -0.3332,  0.0517,\n",
       "           0.2547,  0.2556,  0.0702,  0.2077, -0.2388,  0.0432,  0.2740,\n",
       "           0.1155],\n",
       "         [ 0.0053, -0.2452,  0.1693, -0.1561,  0.0096, -0.2666,  0.0875,\n",
       "           0.1585,  0.2412,  0.1068,  0.1697, -0.2195,  0.1377,  0.0993,\n",
       "           0.0125],\n",
       "         [ 0.0769, -0.0422,  0.1295, -0.1372,  0.0156, -0.3321,  0.0245,\n",
       "           0.2075,  0.1447,  0.1233,  0.1157, -0.2569,  0.2089,  0.0249,\n",
       "           0.0300],\n",
       "         [-0.1507, -0.2629,  0.1344, -0.1698, -0.0832, -0.1477,  0.1191,\n",
       "           0.1807,  0.1812,  0.0761,  0.1205, -0.2088, -0.0027,  0.1596,\n",
       "           0.0796]]], grad_fn=<CatBackward>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_1 = torch.LongTensor(3,5).random_(1,10)\n",
    "length_1 = torch.LongTensor([5,5,5]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_1 = word_encoder(inp_1,length_1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 15])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 15])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_1.squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.zeros(1,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 15])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_1 = torch.cat((out_1.squeeze(),z)).unsqueeze(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 15])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 15])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_s = torch.cat((out,out_1),dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 15])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 15])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 15])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sentenceEncoder(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,repr_size,output_size):\n",
    "        super(sentenceEncoder,self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.gru = nn.GRU(input_size, hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.g2r = nn.Linear(2*hidden_size,repr_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.u_s = nn.Parameter(torch.rand(repr_size)) # sentence context\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        self.r2o = nn.Linear(repr_size,output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, X, X_lengths, batch_size):\n",
    "        self.hidden = self.initHidden(batch_size)\n",
    "\n",
    "        X = rnn.pack_padded_sequence(X, X_lengths, batch_first=True)\n",
    "\n",
    "        X, self.hidden = self.gru(X, self.hidden)\n",
    "\n",
    "        X, _ = torch.nn.utils.rnn.pad_packed_sequence(X, batch_first=True)\n",
    "\n",
    "        X = self.g2r(X)\n",
    "        X = self.tanh(X)\n",
    "        \n",
    "        Y = torch.unbind(X,dim=0)\n",
    "        \n",
    "        Y_1 = torch.Tensor()\n",
    "        for i in range(X_lengths.shape[0]):\n",
    "            x = self.softmax(torch.sum(Y[i][:X_lengths[i].item()]*self.u_s,dim=1)).view(-1,1)\n",
    "            Y_1 = torch.cat((Y_1,torch.sum(Y[i][:X_lengths[i].item()]*x,dim=0).view(1,1,-1)),dim=1)\n",
    "        \n",
    "        output = self.r2o(Y_1)\n",
    "        output = self.sigmoid(output)\n",
    "        \n",
    "        return output\n",
    "        \n",
    "    def initHidden(self,batch_size):\n",
    "         return torch.zeros(2, batch_size, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_enc = sentenceEncoder(15,20,15,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_s_length = torch.LongTensor([4,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5318, 0.5057],\n",
       "         [0.5416, 0.5014]]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_enc(input_s,inp_s_length,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inp (4,5), inp_1 (3,5), length (5,5,4,3), length_1 (5,5,5), \n",
    "len_sent = torch.LongTensor([4,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sent = torch.cat((inp,inp_1),dim=0)\n",
    "all_sent_len = torch.cat((length,length_1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sent[2,4] = 0\n",
    "all_sent[3,4] = 0\n",
    "all_sent[3,3] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3, 3, 8, 3, 2],\n",
       "        [8, 7, 7, 5, 8],\n",
       "        [9, 7, 6, 9, 0],\n",
       "        [1, 1, 8, 0, 0],\n",
       "        [8, 2, 2, 4, 3],\n",
       "        [7, 5, 1, 3, 8],\n",
       "        [1, 7, 6, 7, 3]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 5, 4, 3, 5, 5, 5])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_sent_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.LongTensor([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenating sentences from all reviews in a batch and then sorting them based on length would change the order\n",
    "# to keep track of the sequence of the sentences we need to remember the original mapping\n",
    "# this routine keeps track of the sequence of the sentences among all the reviews\n",
    "# need when passing to the sentence encoder...\n",
    "\n",
    "def originalMap(indices):  \n",
    "    count = 0\n",
    "    m = {}\n",
    "    for i in indices:\n",
    "        m[i.item()] = count\n",
    "        count+=1\n",
    "\n",
    "    m_p = []    \n",
    "    for i,val in sorted(m.items(),key=lambda x:x[0]):\n",
    "        m_p.append(val)\n",
    "\n",
    "    return torch.LongTensor(m_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sortbylength(all_sent,all_sent_len):\n",
    "    sorted_lengths, indices = torch.sort(all_sent_len,descending=True)\n",
    "    mapped_index = originalMap(indices)\n",
    "    return all_sent[torch.LongTensor(indices),:],sorted_lengths,mapped_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,X_lengths,mapped_index = sortbylength(all_sent,all_sent_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = (all_sent,all_sent_len,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 5])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_sent.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(wordEnc,sentEnc,data_train,batch_size=7,epochs=1,learning_rate=0.001):\n",
    "    \n",
    "    wordEnc_optimizer = optim.Adam(wordEnc.parameters(), lr=learning_rate)\n",
    "    sentEnc_optimizer = optim.Adam(sentEnc.parameters(), lr=learning_rate)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for _ in range(epochs):\n",
    "        X,X_lengths,mapped_index = sortbylength(data_train[0],data_train[1])\n",
    "        y = data_train[2]\n",
    "        y = y.view(1,-1)\n",
    "        \n",
    "        sent_vec = wordEnc(X,X_lengths,batch_size)\n",
    "        \n",
    "        sent_vec = sent_vec.squeeze()[mapped_index,:].unsqueeze(dim=0)\n",
    "        \n",
    "        sent_vec = torch.cat((sent_vec.squeeze(),torch.zeros(1,15))).unsqueeze(dim=0)\n",
    "        \n",
    "        sent_vec = sent_vec.view(2,-1,15)\n",
    "        \n",
    "        output = sentEnc(sent_vec,torch.LongTensor([4,3]),2)\n",
    "        \n",
    "        loss = criterion(output,y)\n",
    "        \n",
    "        print(loss)\n",
    "        \n",
    "        wordEnc_optimizer.zero_grad()\n",
    "        sentEnc_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        sentEnc_optimizer.step()\n",
    "        wordEnc_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordEnc = wordEncoder(10,15,20,15,0)\n",
    "sentEnc = sentenceEncoder(15,20,15,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,X_lengths,mapped_index = sortbylength(data_train[0],data_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 5, 20])\n"
     ]
    }
   ],
   "source": [
    "out = wordEnc(X,X_lengths,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 15])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_1 = torch.cat((out.squeeze(),torch.zeros(1,15))).unsqueeze(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 15])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_1 = out_1.view(2,-1,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 15])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5392, 0.5036],\n",
       "         [0.5387, 0.5031]]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_enc(out_1,torch.LongTensor([4,3]),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 5, 20])\n",
      "tensor(0.6927, grad_fn=<NllLoss2DBackward>)\n"
     ]
    }
   ],
   "source": [
    "train(wordEnc,sentEnc,data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
